{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvD_fmQGKwm7"
      },
      "source": [
        "# 1️⃣ Training an Adapter for a Transformer Model\n",
        "\n",
        "In this notebook, we train an adapter for a **RoBERTa** ([Liu et al., 2019](https://arxiv.org/pdf/1907.11692.pdf)) model for sequence classification on a **sentiment analysis** task using the _[Adapters](https://github.com/Adapter-Hub/adapters)_ library and Hugging Face's _Transformers_ library.\n",
        "\n",
        "We train a **[bottleneck adapter](https://docs.adapterhub.ml/methods.html#bottleneck-adapters)** on top of a pre-trained model here. Most of the code is identical to a full fine-tuning setup using _Transformers_.\n",
        "\n",
        "For training, we use the [movie review dataset by Pang and Lee (2005)](http://www.cs.cornell.edu/people/pabo/movie-review-data/). It contains movie reviews from Rotten Tomatoes which are either classified as positive or negative. We download the dataset via Hugging Face's [_Datasets_](https://github.com/huggingface/datasets) library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcuSyuzjKzoW"
      },
      "source": [
        "## Installation\n",
        "\n",
        "First, let's install the required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3P8cRBAXK1fW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qq adapters datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjiwZ8a0K7mJ"
      },
      "source": [
        "## Dataset Preprocessing\n",
        "\n",
        "Before we start to train our adapter, we first prepare the training data. Our training dataset can be loaded via HuggingFace `datasets` using one line of code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LCLS9RYeK83s"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/an/data/github/AITK-673-query-classificator/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'train': 8530, 'validation': 1066, 'test': 1066}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"rotten_tomatoes\")\n",
        "dataset.num_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DnDtZHpK-_t"
      },
      "source": [
        "Every dataset sample has an input text and a binary label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huLjPAKHLA1g",
        "outputId": "cdeee448-f3b6-4fa1-9d93-1248c0ccddad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
              " 'label': 1}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYD2HAvSLFwA"
      },
      "source": [
        "Now, we need to encode all dataset samples to valid inputs for our Transformer model. Since we want to train on `roberta-base`, we load the corresponding `RobertaTokenizer`. Using `dataset.map()`, we can pass the full dataset through the tokenizer in batches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: FlagEmbedding in ./.venv/lib/python3.10/site-packages (1.3.4)\n",
            "Requirement already satisfied: torch>=1.6.0 in ./.venv/lib/python3.10/site-packages (from FlagEmbedding) (2.6.0)\n",
            "Requirement already satisfied: transformers>=4.44.2 in ./.venv/lib/python3.10/site-packages (from FlagEmbedding) (4.48.3)\n",
            "Requirement already satisfied: datasets>=2.19.0 in ./.venv/lib/python3.10/site-packages (from FlagEmbedding) (3.5.0)\n",
            "Requirement already satisfied: accelerate>=0.20.1 in ./.venv/lib/python3.10/site-packages (from FlagEmbedding) (1.6.0)\n",
            "Requirement already satisfied: sentence_transformers in ./.venv/lib/python3.10/site-packages (from FlagEmbedding) (4.1.0)\n",
            "Requirement already satisfied: peft in ./.venv/lib/python3.10/site-packages (from FlagEmbedding) (0.15.2)\n",
            "Requirement already satisfied: ir-datasets in ./.venv/lib/python3.10/site-packages (from FlagEmbedding) (0.5.10)\n",
            "Requirement already satisfied: sentencepiece in ./.venv/lib/python3.10/site-packages (from FlagEmbedding) (0.2.0)\n",
            "Requirement already satisfied: protobuf in ./.venv/lib/python3.10/site-packages (from FlagEmbedding) (6.30.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./.venv/lib/python3.10/site-packages (from accelerate>=0.20.1->FlagEmbedding) (2.2.5)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from accelerate>=0.20.1->FlagEmbedding) (25.0)\n",
            "Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from accelerate>=0.20.1->FlagEmbedding) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in ./.venv/lib/python3.10/site-packages (from accelerate>=0.20.1->FlagEmbedding) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.venv/lib/python3.10/site-packages (from accelerate>=0.20.1->FlagEmbedding) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.10/site-packages (from accelerate>=0.20.1->FlagEmbedding) (0.5.3)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from datasets>=2.19.0->FlagEmbedding) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.10/site-packages (from datasets>=2.19.0->FlagEmbedding) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from datasets>=2.19.0->FlagEmbedding) (0.3.8)\n",
            "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from datasets>=2.19.0->FlagEmbedding) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.10/site-packages (from datasets>=2.19.0->FlagEmbedding) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.10/site-packages (from datasets>=2.19.0->FlagEmbedding) (4.67.1)\n",
            "Requirement already satisfied: xxhash in ./.venv/lib/python3.10/site-packages (from datasets>=2.19.0->FlagEmbedding) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.10/site-packages (from datasets>=2.19.0->FlagEmbedding) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in ./.venv/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in ./.venv/lib/python3.10/site-packages (from datasets>=2.19.0->FlagEmbedding) (3.11.18)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (4.13.2)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.6.0->FlagEmbedding) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers>=4.44.2->FlagEmbedding) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.10/site-packages (from transformers>=4.44.2->FlagEmbedding) (0.21.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in ./.venv/lib/python3.10/site-packages (from ir-datasets->FlagEmbedding) (4.13.4)\n",
            "Requirement already satisfied: inscriptis>=2.2.0 in ./.venv/lib/python3.10/site-packages (from ir-datasets->FlagEmbedding) (2.6.0)\n",
            "Requirement already satisfied: lxml>=4.5.2 in ./.venv/lib/python3.10/site-packages (from ir-datasets->FlagEmbedding) (5.4.0)\n",
            "Requirement already satisfied: trec-car-tools>=2.5.4 in ./.venv/lib/python3.10/site-packages (from ir-datasets->FlagEmbedding) (2.6)\n",
            "Requirement already satisfied: lz4>=3.1.10 in ./.venv/lib/python3.10/site-packages (from ir-datasets->FlagEmbedding) (4.4.4)\n",
            "Requirement already satisfied: warc3-wet>=0.2.3 in ./.venv/lib/python3.10/site-packages (from ir-datasets->FlagEmbedding) (0.2.5)\n",
            "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in ./.venv/lib/python3.10/site-packages (from ir-datasets->FlagEmbedding) (0.2.5)\n",
            "Requirement already satisfied: zlib-state>=0.1.3 in ./.venv/lib/python3.10/site-packages (from ir-datasets->FlagEmbedding) (0.1.9)\n",
            "Requirement already satisfied: ijson>=3.1.3 in ./.venv/lib/python3.10/site-packages (from ir-datasets->FlagEmbedding) (3.3.0)\n",
            "Requirement already satisfied: unlzw3>=0.2.1 in ./.venv/lib/python3.10/site-packages (from ir-datasets->FlagEmbedding) (0.2.3)\n",
            "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (from sentence_transformers->FlagEmbedding) (1.6.1)\n",
            "Requirement already satisfied: scipy in ./.venv/lib/python3.10/site-packages (from sentence_transformers->FlagEmbedding) (1.15.2)\n",
            "Requirement already satisfied: Pillow in ./.venv/lib/python3.10/site-packages (from sentence_transformers->FlagEmbedding) (11.2.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.10/site-packages (from beautifulsoup4>=4.4.1->ir-datasets->FlagEmbedding) (2.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding) (2025.1.31)\n",
            "Requirement already satisfied: cbor>=1.0.0 in ./.venv/lib/python3.10/site-packages (from trec-car-tools>=2.5.4->ir-datasets->FlagEmbedding) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->FlagEmbedding) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets>=2.19.0->FlagEmbedding) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets>=2.19.0->FlagEmbedding) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->datasets>=2.19.0->FlagEmbedding) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn->sentence_transformers->FlagEmbedding) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn->sentence_transformers->FlagEmbedding) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.19.0->FlagEmbedding) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U FlagEmbedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 49228.92it/s]\n",
            "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.626  0.3477]\n",
            " [0.35   0.678 ]]\n"
          ]
        }
      ],
      "source": [
        "from FlagEmbedding import BGEM3FlagModel\n",
        "\n",
        "model = BGEM3FlagModel('BAAI/bge-m3',  \n",
        "                       use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
        "\n",
        "sentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\n",
        "sentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n",
        "               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n",
        "\n",
        "embeddings_1 = model.encode(sentences_1, \n",
        "                            batch_size=12, \n",
        "                            max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n",
        "                            )['dense_vecs']\n",
        "embeddings_2 = model.encode(sentences_2)['dense_vecs']\n",
        "similarity = embeddings_1 @ embeddings_2.T\n",
        "print(similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oDHhicggLHW7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-m3')\n",
        "\n",
        "def encode_batch(batch):\n",
        "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
        "  return tokenizer(batch[\"text\"], max_length=300, truncation=True, padding=\"max_length\")\n",
        "\n",
        "# Encode the input data\n",
        "dataset = dataset.map(encode_batch, batched=True)\n",
        "# The transformers model expects the target class column to be named \"labels\"\n",
        "dataset = dataset.rename_column(original_column_name=\"label\", new_column_name=\"labels\")\n",
        "# Transform to pytorch tensors and only output the required columns\n",
        "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwl-pv89LJfa"
      },
      "source": [
        "Now we're ready to train our model..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgykXOBHLLFK"
      },
      "source": [
        "## Training\n",
        "\n",
        "We use a pre-trained RoBERTa model checkpoint from the Hugging Face Hub. We load it with [`AutoAdapterModel`](https://docs.adapterhub.ml/classes/models/auto.html), a class unique to `adapters`. In addition to regular _Transformers_ classes, this class comes with all sorts of adapter-specific functionality, allowing flexible management and configuration of multiple adapters and prediction heads. [Learn more](https://docs.adapterhub.ml/prediction_heads.html#adaptermodel-classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hf_xet in ./.venv/lib/python3.10/site-packages (1.0.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install hf_xet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lENbN034LMOk"
      },
      "outputs": [],
      "source": [
        "from transformers import XLMRobertaConfig\n",
        "from adapters import AutoAdapterModel\n",
        "\n",
        "config = XLMRobertaConfig.from_pretrained(\n",
        "    \"BAAI/bge-m3\",\n",
        "    num_labels=2,\n",
        ")\n",
        "model = AutoAdapterModel.from_pretrained(\n",
        "    \"BAAI/bge-m3\",\n",
        "    config=config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY4Xiy9iLOVO"
      },
      "source": [
        "**Here comes the important part!**\n",
        "\n",
        "We add a new adapter to our model by calling `add_adapter()`. We pass a name (`\"rotten_tomatoes\"`) and an adapter configuration. `\"seq_bn\"` denotes a [sequential bottleneck adapter](https://docs.adapterhub.ml/methods.html#bottleneck-adapters) configuration.\n",
        "_Adapters_ supports a diverse range of different adapter configurations. For example, `config=\"lora\"` can be passed for training a [LoRA](https://docs.adapterhub.ml/methods.html#lora) adapter, `config=\"prefix_tuning\"` for [prefix tuning](https://docs.adapterhub.ml/methods.html#prefix-tuning) or `config=\"loreft\"` for [LoReFT](https://docs.adapterhub.ml/methods.html#reft). You can find all currently supported configs [here](https://docs.adapterhub.ml/methods.html).\n",
        "\n",
        "Next, we add a binary classification head. It's convenient to give the prediction head the same name as the adapter. This allows us to activate both together in the next step. The `train_adapter()` method does two things:\n",
        "\n",
        "1. It freezes all weights of the pre-trained model, so only the adapter weights are updated during training.\n",
        "2. It activates the adapter and the prediction head such that both are used in every forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "saGtTGogLPVf"
      },
      "outputs": [],
      "source": [
        "# Add a new adapter\n",
        "model.add_adapter(\"rotten_tomatoes\", config=\"seq_bn\")\n",
        "# model.add_adapter(\"rotten_tomatoes\", config=\"lora\")\n",
        "# model.add_adapter(\"prefix_tuning\", config=\"loreft\")\n",
        "# Alternatively, e.g.:\n",
        "# model.add_adapter(\"rotten_tomatoes\", config=\"lora\")\n",
        "\n",
        "# Add a matching classification head\n",
        "model.add_classification_head(\n",
        "    \"rotten_tomatoes\",\n",
        "    num_labels=2,\n",
        "    id2label={ 0: \"👎\", 1: \"👍\"}\n",
        "  )\n",
        "\n",
        "# Activate the adapter\n",
        "model.train_adapter(\"rotten_tomatoes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyLopENcLSEq"
      },
      "source": [
        "For training an adapter, we make use of the `AdapterTrainer` class built-in into _Adapters_. This class is largely identical to _Transformer_'s `Trainer`, with some helpful tweaks e.g. for checkpointing only adapter weights.\n",
        "\n",
        "We configure the training process using a `TrainingArguments` object and define a method that will calculate the evaluation accuracy in the end. We pass both, together with the training and validation split of our dataset, to the trainer instance.\n",
        "\n",
        "**Note the differences in hyperparameters compared to full fine-tuning.** Adapter training usually requires a few more training epochs than full fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QoTEQbhQLTGB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers import TrainingArguments, EvalPrediction\n",
        "from adapters import AdapterTrainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    learning_rate=1e-4,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=12,\n",
        "    per_device_eval_batch_size=12,\n",
        "    logging_steps=200,\n",
        "    output_dir=\"./training_output\",\n",
        "    overwrite_output_dir=True,\n",
        "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "def compute_accuracy(p: EvalPrediction):\n",
        "  preds = np.argmax(p.predictions, axis=1)\n",
        "  return {\"acc\": (preds == p.label_ids).mean()}\n",
        "\n",
        "trainer = AdapterTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    compute_metrics=compute_accuracy,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtSsZNJsLVk4"
      },
      "source": [
        "Start the training 🚀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416,
          "referenced_widgets": [
            "2da09ed5df2d4357b8b7029ca599c5e2"
          ]
        },
        "id": "isGxmG9LLnhs",
        "outputId": "511f8d82-ef8e-47c4-8dcd-a754f4a65cfe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='711' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 12/711 00:16 < 19:25, 0.60 it/s, Epoch 0.02/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/data/github/AITK-673-query-classificator/.venv/lib/python3.10/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/data/github/AITK-673-query-classificator/.venv/lib/python3.10/site-packages/transformers/trainer.py:2536\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BLPys_0LuI5"
      },
      "source": [
        "Looks good! Let's evaluate our adapter on the validation split of the dataset to see how well it learned:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "1d8d71a940884516978427285a4927fc"
          ]
        },
        "id": "hnct-N6dLvOd",
        "outputId": "a0b951c7-16ac-4958-8b6d-df8298157f2d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='89' max='89' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [89/89 01:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.3078943192958832,\n",
              " 'eval_acc': 0.8705440900562852,\n",
              " 'eval_runtime': 61.3447,\n",
              " 'eval_samples_per_second': 17.377,\n",
              " 'eval_steps_per_second': 1.451,\n",
              " 'epoch': 1.0}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTtjRB_VNJbL"
      },
      "source": [
        "We can put our trained model into a _Transformers_ pipeline to be able to make new predictions conveniently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-BYe1jeNK98",
        "outputId": "1df24b91-cc26-4511-b496-10bae08b610f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "The model 'XLMRobertaAdapterModel' is not supported for . Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DiffLlamaForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'Gemma2ForSequenceClassification', 'GlmForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'JambaForSequenceClassification', 'JetMoeForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'ModernBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NemotronForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'Phi3ForSequenceClassification', 'PhimoeForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'Qwen2MoeForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification', 'ZambaForSequenceClassification'].\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': '👍', 'score': 0.9606837630271912}]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=training_args.device.index)\n",
        "\n",
        "classifier(\"This is awesome!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANVQrhIsNPOu"
      },
      "source": [
        "At last, we can also extract the adapter from our model and separately save it for later reuse. Note the size difference compared to a full model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50FhEtBrNP_C",
        "outputId": "f50f64da-cf6e-4456-de35-d0ceaebbd677"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 5,7M\n",
            "-rw-rw-r-- 1 an an 1,1K апр 23 09:04 adapter_config.json\n",
            "-rw-rw-r-- 1 an an  452 апр 23 09:04 head_config.json\n",
            "-rw-rw-r-- 1 an an 3,5M апр 23 09:04 pytorch_adapter.bin\n",
            "-rw-rw-r-- 1 an an 2,3M апр 23 09:04 pytorch_model_head.bin\n"
          ]
        }
      ],
      "source": [
        "model.save_adapter(\"./final_adapter\", \"rotten_tomatoes\")\n",
        "\n",
        "!ls -lh final_adapter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sAJKaQeNREg"
      },
      "source": [
        "**Share your work!**\n",
        "\n",
        "The final step after successful training is to share our adapter with the world!\n",
        "_Adapters_ seamlessly integrates with the [Hugging Face Model Hub](https://huggingface.co/models), so you can publish your trained adapter with a single method call:\n",
        "\n",
        "**Important:** Make sure you're properly authenticated with your Hugging Face account before running this method. You can log in by running `huggingface-cli login` on your terminal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MQqlgcJG8LY"
      },
      "outputs": [],
      "source": [
        "model.push_adapter_to_hub(\n",
        "    \"my-awesome-adapter\",\n",
        "    \"rotten_tomatoes\",\n",
        "    datasets_tag=\"rotten_tomatoes\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czah-83hG8LY"
      },
      "source": [
        "This will create a repository _my-awesome-adapter_ under your username, generate a default adapter card as README.md and upload the adapter named `rotten_tomatoes` together with the adapter card to the new repository. [Learn more](https://docs.adapterhub.ml/huggingface_hub.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOhAzIvyG8LY"
      },
      "source": [
        "➡️ Continue with [the next Colab notebook](https://colab.research.google.com/github/Adapter-Hub/adapters/blob/main/notebooks/02_Adapter_Inference.ipynb) to learn how to use adapters from the Hub."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
